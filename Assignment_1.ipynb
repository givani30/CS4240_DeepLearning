{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/givani30/CS4240_DeepLearning/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssLI7a7GKzgB"
   },
   "source": [
    "# CS4240 Deep Learning - Assignment 1\n",
    "\n",
    "*These lab assignments are new in the CS4240 Deep Learning course. We'd like to hear what you think!*\n",
    "\n",
    "*Please post any feedback you have on Brightspace. Thanks!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rSnXZSgqpfk"
   },
   "source": [
    "To start working on the assignment in Colab, save a copy on your Google Drive (`File` $\\rightarrow$ `Save a copy in Drive`).\n",
    "\n",
    "To work on the assignments locally, configure your conda environment (see instructions on Brightspace) and download this assignment as an IPython Notebook (`File` $\\rightarrow$ `Download .ipynb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdx7o5N2S2Tn"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment you will be introduced to some basic functionality in PyTorch, you will write your own fully connected neural network layers and non-linear activation functions and will use those to solve a simple XOR classification problem.\n",
    "\n",
    "**Prerequisites:**\n",
    "* Basic knowledge of Python and Numpy. Recommended tutorial for Python and Numpy [here](https://cs231n.github.io/python-numpy-tutorial/).\n",
    "\n",
    "**Learning objectives:**\n",
    "* Using basic tensor operations in PyTorch;\n",
    "* Understanding and implementing the forward pass of a fully connected layer and the ReLU and Sigmoid non-linear activation functions in PyTorch;\n",
    "* Using basic PyTorch layers to build a simple neural network;\n",
    "* Understanding and calculating the MSE loss;\n",
    "* Understanding the training loop.\n",
    "\n",
    "We will share the solutions one week after the assignments are published. Throughout the assignment you will validate your code by comparing the outputs of your own implementations to the equivalent PyTorch implementations.\n",
    "\n",
    "**For your own implementations you may only use basic tensor operations from the `torch` module: no `torch.nn`, `torch.nn.F` or others. There is a small introduction into pytorch tensors in A1.0**\n",
    "\n",
    "\n",
    "When answering coding questions make sure to write your own code within the designated part of the code block as illustrated here:\n",
    "```python \n",
    "#############################################################################\n",
    "#                       TODO: Implement function x                          #\n",
    "#############################################################################\n",
    "# Replace \"pass\" statement with your code\n",
    "pass\n",
    "#############################################################################\n",
    "#                            END OF YOUR CODE                               #\n",
    "#############################################################################\n",
    "```\n",
    "\n",
    "Please pay attention to the question boxes and try to think about them. The boxes are indicated as follows:\n",
    "\n",
    "****\n",
    "**Questions?**\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIvAOzFjhtw6"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeLiiyq-Hnw1"
   },
   "source": [
    "##A1.0 Tensors in Pytorch\n",
    "\n",
    "Throughout the course, you will use the Pytorch library to design and evaluate your models. In those implementations you will (and must) often use Pytorch tensors (ranging from 1D to high-D). You have probably worked with `numpy` arrays quite much. Fortunately, creating and manipulating Pytorch tensors is highly similar. A detailed description of *how* to deal with them is given [here](https://colab.research.google.com/drive/1L9aMXms7ZXqjOktbwtH8H1VY8ixah6yU) (which is in *view* mode; for editing you must copy paste it).\n",
    "\n",
    "It is encouraged to have a look as it will make solving the lab assignments easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuIHuF2dMzSV"
   },
   "source": [
    "## A1.1 Linear (fully connected) layer\n",
    "\n",
    "In this first exercise, you will 1. implement the linear layer, 2. perform a forward pass on some dummy input, and 3. compare the output tensor to the *Pytorch* implementation of a linear layer.\n",
    "\n",
    "A linear layer is defined as a fully-connected map from some input neurons to some output neurons. Say you have $N_{\\text{in}}$ and $N_{\\text{out}}$ neurons, there are a total of $N_{\\text{in}} \\cdot N_{\\text{out}}$ connections (weigths) and $N_{\\text{out}}$ biases. The fully-connected layer for some input vector $\\mathbf{x} \\in \\mathbb{R}^{N_{\\text{in}}}$, ouput vector $\\mathbf{y}  \\in \\mathbb{R}^{N_{\\text{out}}}$, weighting matrix $\\mathbf{W}  \\in \\mathbb{R}^{N_{\\text{in}} \\times N_{\\text{out}}} $, and bias vector $\\mathbf{b}  \\in \\mathbb{R}^{N_{\\text{out}}}$ is given by:\n",
    "\n",
    "$$ \\mathbf{y} = \\mathbf{x}^T \\mathbf{W} + \\mathbf{b}$$\n",
    "\n",
    "****\n",
    "**What does $\\mathbf{W}_{ij}$ represent exactly? What is the size of $\\mathbf{W}$? And why is $\\mathbf{x}$ tranposed?** \n",
    "The weight ij represents the weight from input i to output j. It can be seen as the relation between these two, if input i increasen by 1, output j increases by $W_{ij}$\n",
    "\n",
    "Because the weighing matrix is defined as $\\mathbf{W}  \\in \\mathbb{R}^{N_{\\text{in}} \\times N_{\\text{out}}} $, The mapping from the input which is  $\\mathbf{x} \\in \\mathbb{R}^{N_{\\text{in}}}$ to output  $\\mathbf{y}  \\in \\mathbb{R}^{N_{\\text{out}}}$ is obtained by premultiplying with the transposed, as else the dimensions would be incompatible\n",
    "****\n",
    "\n",
    "We will implement the linear layer as a Python *object*. Keep in mind that we want to be able to process multiple samples in one forward pass, so $\\mathbf{x}$ will include a batch dimension such that $\\mathbf{x} \\in \\mathbb{R}^{\\text{batch},N_{\\text{in}}}$ and $\\mathbf{x}\\mathbf{W}$ will become a matrix multiplication.\n",
    "\n",
    "****\n",
    "**Why would you want to do that? Hint: think bigger, literally ;)!** \n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWcqUhVgfR5A"
   },
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    \"\"\"\n",
    "    Fully connected layer.\n",
    "    \n",
    "    Args:\n",
    "        in_features: number of input features\n",
    "        out_features: number of output features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        ########################################################################\n",
    "        #      TODO: Define placeholder tensors for layer weight and bias.     #\n",
    "        #       The placeholder tensors should have the correct dimension      #\n",
    "        #        according to the in_features and out_features variables.      #\n",
    "        #                    Note: no for loops are needed!                    #\n",
    "        ########################################################################\n",
    "        self.bias = torch.empty(out_features)\n",
    "        self.weight = torch.empty([in_features,out_features])\n",
    "        ########################################################################\n",
    "        #                         END OF YOUR CODE                             #\n",
    "        ########################################################################\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self, std=0.1):\n",
    "        \"\"\"\n",
    "        Initialize layer parameters. Sample weight from Gaussian distribution\n",
    "        and bias uniform distribution.\n",
    "        \n",
    "        Args:\n",
    "            std: Standard deviation of Gaussian distribution (default: 0.1)\n",
    "        \"\"\"\n",
    "        self.weight = std*torch.randn_like(self.weight)\n",
    "        self.bias = torch.rand_like(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of Linear layer: multiply input tensor by weights and add\n",
    "        bias.\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor\n",
    "\n",
    "        Returns:\n",
    "            y: output tensor\n",
    "        \"\"\"\n",
    "        ########################################################################\n",
    "        #                  TODO: Implement this function                       #\n",
    "        ########################################################################\n",
    "        y=torch.matmul(x,self.weight)+self.bias\n",
    "        ########################################################################\n",
    "        #                         END OF YOUR CODE                             #\n",
    "        ########################################################################\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtE7ytKSFBst"
   },
   "source": [
    "Now test the forward pass of the layer on some dummy input $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "****\n",
    "**What will be the shape of output tensor $\\mathbf{y}$?**\n",
    "Size will be n_samples*out_features\n",
    "****"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSALY1wPFGXZ"
   },
   "outputs": [],
   "source": [
    "# Define layer dimensions and dummy input\n",
    "n_samples, in_features, out_features = 2, 3, 4\n",
    "# Make random input tensor of dimensions [n_samples, in_features]\n",
    "x = torch.randn((n_samples, in_features))\n",
    "\n",
    "########################################################################\n",
    "#          TODO: Create a layer from the Linear object class           #\n",
    "#                     above and do a forward pass                      #\n",
    "########################################################################\n",
    "layer=Linear(in_features,out_features)\n",
    "y=layer.forward(x)\n",
    "\n",
    "########################################################################\n",
    "#                         END OF YOUR CODE                             #\n",
    "########################################################################\n",
    "\n",
    "# What will be the shape of output tensor y?\n",
    "print('Shape of ouput tensor y:', y.shape)\n",
    "print('Shape of output tensor y correct: ', y.shape == torch.Size([n_samples, out_features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L5vbHyFIE9m"
   },
   "source": [
    "If the forward pass has not returned any errors we can compare our implementation to the PyTorch linear layer [[docs](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)] from `torch.nn`. We do so by initializing a `nn.Linear` layer and setting the `weight` and `bias` to the same values as in our own linear layer.\n",
    "\n",
    "PyTorch layers store their parameters as a `Parameter`, which is a `Tensor` subclass with some special properties [[docs](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html)]. We therefore need to wrap our `layer.weight` and `layer.bias` in a `nn.Parameter` when using in `nn.Linear`. Moreover, the `weight` tensor is transposed in `nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xa-_W-MgFJzN"
   },
   "outputs": [],
   "source": [
    "# Create Linear layer from torch.nn module\n",
    "torch_layer = nn.Linear(in_features, out_features)\n",
    "\n",
    "# Load the parameters from our layer into the Pytorch layer\n",
    "torch_layer.weight = nn.Parameter(layer.weight.T) # transpose weight by .T\n",
    "torch_layer.bias = nn.Parameter(layer.bias)\n",
    "\n",
    "# Perform forward pass\n",
    "torch_y = torch_layer(x)\n",
    "\n",
    "# What will be the shape of output tensor torch_y?\n",
    "print('Shape of ouput tensor torch_y:', torch_y.shape)\n",
    "print('Shape of output tensor y correct: ', y.shape == torch.Size([n_samples, out_features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjE7DIp9Ltut"
   },
   "source": [
    "We now compare the outputs of both implementations using `torch.allclose` [[docs](https://pytorch.org/docs/stable/generated/torch.allclose.html)], which returns `True` if all elements in both tensors are sufficiently \"close\" to each other (see documentation for what that means exactly).\n",
    "\n",
    "Your forward implementation of the linear layer is *correct* if `True` is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DMC0nrYNkIf"
   },
   "outputs": [],
   "source": [
    "# Compare outputs using torch.allclose\n",
    "outputs_same = torch.allclose(y, torch_y)\n",
    "print('Outputs identical: ', outputs_same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N41y5ntsMw40"
   },
   "source": [
    "## A1.2 Non-linear activation functions\n",
    "\n",
    "As the XOR problem cannot be solved by a simple linear classifier we require our neural network to be able to learn non-linear functions. This is where non-linear activation functions come into play. In this assignment we will implement the forward passes of two popular activation functions, namely the Rectified Linear Unit\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x)=\\max(0,x)\n",
    "$$\n",
    "\n",
    "and the Sigmoid function\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+\\exp(-x)}\n",
    "$$ \n",
    "\n",
    "To verify your implementation you will again compare against their PyTorch counterparts.\n",
    "\n",
    "****\n",
    "**Can you still remember from the lectures what the (dis)advantages are for each activation function? Think in terms of efficiency and effectiveness.** \n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOphZSwhMqn6"
   },
   "outputs": [],
   "source": [
    "class ReLU(object):\n",
    "    \"\"\"\n",
    "    ReLU non-linear activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of ReLU non-linear activation function: y=max(0,x).\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor\n",
    "\n",
    "        Returns:\n",
    "            y: output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        ########################################################################\n",
    "        #                  TODO: Implement this function                       #\n",
    "        ########################################################################\n",
    "\n",
    "        y=torch.maximum(torch.zeros_like(x),x)\n",
    "\n",
    "        ########################################################################\n",
    "        #                         END OF YOUR CODE                             #\n",
    "        ########################################################################\n",
    "\n",
    "        return y\n",
    "\n",
    "class Sigmoid(object):\n",
    "    \"\"\"\n",
    "    Sigmoid non-linear activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of Sigmoid non-linear activation function: y=1/(1+exp(-x)).\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor\n",
    "\n",
    "        Returns:\n",
    "            y: output tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        ########################################################################\n",
    "        #                  TODO: Implement this function                       #\n",
    "        ########################################################################\n",
    "\n",
    "        y=1/(1+torch.exp(-x))\n",
    "\n",
    "        ########################################################################\n",
    "        #                         END OF YOUR CODE                             #\n",
    "        ########################################################################\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtOu5BCKQvky"
   },
   "source": [
    "Test the forward pass of both non-linearities using the dummy input from before.\n",
    "\n",
    "****\n",
    "**Again: what will be shapes of the output tensors?** \n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-sCWIhNOetw"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#    TODO: Perform a forward pass for the ReLU and Sigmoid layers.     #\n",
    "########################################################################\n",
    "\n",
    "layer_sigmoid=Sigmoid()\n",
    "layer_relu=ReLU()\n",
    "y_relu=layer_relu.forward(x)\n",
    "y_sigmoid=layer_sigmoid.forward(x)\n",
    "\n",
    "########################################################################\n",
    "#                         END OF YOUR CODE                             #\n",
    "########################################################################\n",
    "\n",
    "# What will be the shapes of output tensors y_relu and y_sigmoid?\n",
    "print('Shape of ouput tensors y_relu and y_sigmoid:', y_relu.shape, y_sigmoid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hiL-1zwRh_G"
   },
   "source": [
    "Now perform a forward pass with the ReLU and Sigmoid activation functions from `torch.nn` and compare the outputs to your implementation.\n",
    "\n",
    "A list of all available non-linearities in PyTorch can be found [[here](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCpv6e_bR6Dg"
   },
   "outputs": [],
   "source": [
    "# ReLU forward pass\n",
    "torch_relu = nn.ReLU()\n",
    "torch_y_relu = torch_relu(x)\n",
    "\n",
    "# Sigmoid forward pass\n",
    "torch_sigmoid = nn.Sigmoid()\n",
    "torch_y_sigmoid = torch_sigmoid(x)\n",
    "\n",
    "# Compare outputs using torch.allclose\n",
    "outputs_same = torch.allclose(y_relu, torch_y_relu)\n",
    "print('ReLU outputs identical: ', outputs_same)\n",
    "outputs_same = torch.allclose(y_sigmoid, torch_y_sigmoid)\n",
    "print('Sigmoid outputs identical: ', outputs_same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7bn6PyoNF8r"
   },
   "source": [
    "## A1.3 Network class\n",
    "\n",
    "We will now create a base class for our neural network that will make it possible to stack individual layers on each other. For now we will only implement the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vi8luwCMgobr"
   },
   "outputs": [],
   "source": [
    "class Net(object):\n",
    "    \"\"\"\n",
    "    Neural network object containing layers.\n",
    "    \n",
    "    Args:\n",
    "        layers: list of layers in neural network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def reset_params(self, std=0.1):\n",
    "        \"\"\"\n",
    "        Reset network parameters. Applies `init_params` to all layers with\n",
    "        learnable parameters.\n",
    "        \n",
    "        Args:\n",
    "            std: Standard deviation of Gaussian distribution (default: 0.1)\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'init_params'):\n",
    "                layer.init_params(std=std)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass through all layers of the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor\n",
    "\n",
    "        Returns:\n",
    "            x: output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        ########################################################################\n",
    "        #                  TODO: Implement this function                       #\n",
    "        ########################################################################\n",
    "        for layer in self.layers:\n",
    "            x=layer.forward(x)\n",
    "\n",
    "        ########################################################################\n",
    "        #                         END OF YOUR CODE                             #\n",
    "        ########################################################################\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xLhaGmqNdAw"
   },
   "source": [
    "We will create a simple 2-layer network with ReLU non-linearity and test the forward pass using the same dummy input as before.\n",
    "\n",
    "****\n",
    "**What will be the shape of the output tensor?**\n",
    "N_samples by output size of final layer\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5k4uPheAjLsY"
   },
   "outputs": [],
   "source": [
    "hidden_dim = 5\n",
    "\n",
    "# Define and initialize layers\n",
    "layers = [Linear(in_features, hidden_dim),\n",
    "          ReLU(),\n",
    "          Linear(hidden_dim, out_features)]\n",
    "\n",
    "# Initialize network\n",
    "net = Net(layers)\n",
    "\n",
    "# Do forward pass\n",
    "y = net.forward(x)\n",
    "\n",
    "# What will be the shape of output tensor y?\n",
    "print('Shape of output tensor y:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LMryIa6V7Zd"
   },
   "source": [
    "We will now create the same neural network in PyTorch. PyTorch uses the `nn.Module` base class for neural network architectures, which is similar to the `Net` object we have just defined. However, other than in the `Net` class, you have to define all layers inside the network definition.\n",
    "\n",
    "This is an important exercise, as this is how you will define all your future models in PyTorch.\n",
    "\n",
    "You can print a PyTorch `module` to see all sub-modules (i.e. layers) in the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mln6iWyrP1NF"
   },
   "outputs": [],
   "source": [
    "# We will call the network TorchNet\n",
    "class TorchNet(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch neural network. Network layers are defined in __init__ and forward\n",
    "    pass implemented in forward.\n",
    "    \n",
    "    Args:\n",
    "        in_features: number of features in input layer\n",
    "        hidden_dim: number of features in hidden dimension\n",
    "        out_features: number of features in output layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_dim, out_features):\n",
    "        super(TorchNet, self).__init__()\n",
    "        \n",
    "        ########################################################################\n",
    "        #      TODO: Define all layers that will be used in this network.      #\n",
    "        ########################################################################\n",
    "\n",
    "        self.layer1=nn.Linear(in_features,hidden_dim)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.layer2=nn.Linear(hidden_dim,out_features)\n",
    "\n",
    "        ########################################################################\n",
    "        #                         END OF YOUR CODE                             #\n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass through all layers of the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor\n",
    "\n",
    "        Returns:\n",
    "            x: output tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        ########################################################################\n",
    "        #                  TODO: Implement this function                       #\n",
    "        ########################################################################\n",
    "\n",
    "        x=self.layer1.forward(x)\n",
    "        x=self.relu.forward(x)\n",
    "        x=self.layer2.forward(x)\n",
    "\n",
    "        ########################################################################\n",
    "        #                         END OF YOUR CODE                             #\n",
    "        ########################################################################\n",
    "\n",
    "        return x\n",
    "\n",
    "# Initialize Pytorch network\n",
    "torch_net = TorchNet(in_features, hidden_dim, out_features)\n",
    "print(torch_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQAFYgOtXTFX"
   },
   "source": [
    "We will now again compare the outputs of the two networks. Therefore we again need to load the weights from our network into the PyTorch network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBYiJnRwQXqB"
   },
   "outputs": [],
   "source": [
    "# Load the parameters from our model into the Pytorch model\n",
    "torch_net.layer1.weight = nn.Parameter(net.layers[0].weight.t()) # transpose weight by .t()\n",
    "torch_net.layer1.bias = nn.Parameter(net.layers[0].bias)\n",
    "torch_net.layer2.weight = nn.Parameter(net.layers[2].weight.t()) # transpose weight by .t()\n",
    "torch_net.layer2.bias = nn.Parameter(net.layers[2].bias)\n",
    "\n",
    "# Perform forward pass\n",
    "torch_y = torch_net(x)\n",
    "\n",
    "# What will be the shape of output tensor torch_y?\n",
    "print('Shape of ouput tensor y:', torch_y.shape)\n",
    "\n",
    "# Compare outputs using torch.allclose\n",
    "outputs_same = torch.allclose(y, torch_y)\n",
    "print('Network outputs identical: ', outputs_same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKrDW462NvNQ"
   },
   "source": [
    "## A1.4: Introducing the XOR problem\n",
    "\n",
    "To introduce the XOR classification problem that we will aim to solve in this assignment using a neural network we'll first generate and visualize the data points. \n",
    "\n",
    "The `plot` function will also be used to visualize the decision boundaries of your neural network.\n",
    "\n",
    "The XOR problem consists of 4 data points belonging to 2 classes which cannot be separated by a linear decision boundary. \n",
    "\n",
    "| x0   | x1   | y    |\n",
    "| ---- | ---- | ---- |\n",
    "| 0    | 0    | 0    |\n",
    "| 0    | 1    | 1    |\n",
    "| 1    | 0    | 1    |\n",
    "| 1    | 1    | 0    |\n",
    "\n",
    "The class labels `y` are [one-hot encoded](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics), i.e. for each class a binary value indicates whether the sample belongs to it or not. For instance, given classes `1,2,3,4,5` the one-hot encoding of class `4` is given by `[0,0,0,1,0]`. One-hot encoding is a natural way to represent class labels in a classification task since a neural network outputs a class probability vector. `[0,0,0,1,0]` then simply corresponds to a 0% chance of the sample belonging the classes `1,2,3,5` and a 100% chance of it belonging to class `4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lR9D37H6JytZ"
   },
   "outputs": [],
   "source": [
    "x_xor = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_xor = torch.tensor([[1, 0], [0, 1], [0, 1], [1, 0]]) # one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ll4xMtgPKpSc"
   },
   "outputs": [],
   "source": [
    "def plot(x, y, net=None):\n",
    "    \"\"\"\n",
    "    Plotter function for XOR dataset and classifier boundaries (optional).\n",
    "\n",
    "    Args:\n",
    "        x: Nx2 dimensional data\n",
    "        y: N dimensional labels\n",
    "        net: Model which has a forward function\n",
    "    \"\"\"\n",
    "    # Convert one-hot to class id\n",
    "    y = torch.argmax(y, dim=1)\n",
    "\n",
    "    # Plot decision boundary if net is given\n",
    "    if net:\n",
    "        h = 0.005\n",
    "        x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "        y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "\n",
    "        xx, yy = torch.meshgrid(torch.arange(x_min, x_max, h),\n",
    "                                torch.arange(y_min, y_max, h))\n",
    "        \n",
    "        in_tensor = torch.cat((xx.reshape((-1,1)), yy.reshape((-1,1))), dim=1)\n",
    "\n",
    "        z = net.forward(in_tensor)\n",
    "        z = torch.argmax(z, dim=1)\n",
    "        z = z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, z, cmap=plt.cm.coolwarm)\n",
    "\n",
    "    # Plot data points\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "    plt.title('XOR problem')\n",
    "    plt.xlabel('x0')\n",
    "    plt.ylabel('x1')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "e4yWIa8TjIBX",
    "outputId": "407c83c9-5b3d-49e2-b481-47c73b58d643"
   },
   "outputs": [],
   "source": [
    "# Visualize\n",
    "plot(x_xor, y_xor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9psFGX5JQtq4"
   },
   "source": [
    "We will now use our non-linear neural network to solve the XOR problem.\n",
    "\n",
    "To gain intuition with the transformations in a linear layer you will manually set the correct network parameters. Keep in mind that the network predicts class probabilities, i.e. for each sample it outputs the probability of belonging to class 0 and class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5cj9RQk_pmv"
   },
   "outputs": [],
   "source": [
    "in_features, hidden_dim, out_features = 2, 2, 2\n",
    "\n",
    "# Define and initialize layers\n",
    "layers = [Linear(in_features, hidden_dim),\n",
    "          ReLU(),\n",
    "          Linear(hidden_dim, out_features)]\n",
    "\n",
    "# Initialize network\n",
    "net = Net(layers)\n",
    "\n",
    "########################################################################\n",
    "#   TODO: Manually set network parameters to solve the XOR problem.    #\n",
    "########################################################################\n",
    "\n",
    "net.layers[0].weight = torch.FloatTensor([[-1,1],[-1,1]])\n",
    "net.layers[0].bias = torch.FloatTensor([1,-1])\n",
    "net.layers[2].weight = torch.FloatTensor([[1,-1],[1,-1]])\n",
    "net.layers[2].bias = torch.FloatTensor([0,1])\n",
    "\n",
    "########################################################################\n",
    "#                         END OF YOUR CODE                             #\n",
    "########################################################################\n",
    "\n",
    "# Forward pass\n",
    "y_pred = net.forward(x_xor)\n",
    "print(y_pred)\n",
    "\n",
    "# Show decision boundary\n",
    "plot(x_xor, y_xor, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83GFFkfiHq1O"
   },
   "source": [
    "We will now increase the size of the hidden representation, i.e. the number of features between the first and second layer, from 2 to 5. Adjust the weights and biases in such a way that this new network yields exactly the same solution as the smaller network. It should be a fairly simple and straightforward modification.\n",
    "\n",
    "*Hint: use the previous solution as a starting point.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "iu5xCxbSFOiz",
    "outputId": "c36ec0c0-6f78-49bd-c157-7171df3feb8e"
   },
   "outputs": [],
   "source": [
    "in_features, hidden_dim, out_features = 2, 5, 2\n",
    "\n",
    "# Define and initialize layers\n",
    "layers = [Linear(in_features, hidden_dim),\n",
    "          ReLU(),\n",
    "          Linear(hidden_dim, out_features)]\n",
    "\n",
    "# Initialize network\n",
    "net = Net(layers)\n",
    "\n",
    "########################################################################\n",
    "#   TODO: Manually set network parameters to solve the XOR problem.    #\n",
    "########################################################################\n",
    "\n",
    "pass\n",
    "\n",
    "########################################################################\n",
    "#                         END OF YOUR CODE                             #\n",
    "########################################################################\n",
    "\n",
    "# Forward pass\n",
    "y_pred = net.forward(x_xor)\n",
    "\n",
    "# Show decision boundary\n",
    "plot(x_xor, y_xor, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1PAklq65pew"
   },
   "source": [
    "## A1.5 A first introduction to the training loop\n",
    "\n",
    "Of course the power of a neural network lies in the fact that it is able to learn from data. We therefore now introduce the training loop. Roughly speaking the training loop consists of the following steps:\n",
    "```\n",
    "while not converged:\n",
    " 1. feed training sample to network to predict output (forward step)\n",
    " 2. compare prediction to label (compute loss)\n",
    " 3. use comparison to update network parameters (backward step)\n",
    "```\n",
    "Since we have not yet implemented the backpropagation algorithm we will attempt to optimize our neural network in a rather primitive fashion by randomly generating weights until the problem is solved, i.e. all data points are classified correctly.\n",
    "\n",
    "Even though a loss function is not required in our case, we will start by implementing the mean squared error loss to act as a heuristic. Given prediction $\\hat{y}$ and groud-truth $y$ the MSE loss is defined as:\n",
    "$$\\text{MSE}(\\hat{y},y)= \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}-y)^2 $$\n",
    "\n",
    "Again keep in mind that $\\hat{y}$ is a class probability vector and $y$ is a one-hot encoded representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLZE2-Pe0zkq"
   },
   "outputs": [],
   "source": [
    "def MSELoss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes mean squared error loss between y_true and y_pred.\n",
    "\n",
    "    Args:\n",
    "      y_true: Tensor containing true labels.\n",
    "      y_pred: Tensor containing predictions.\n",
    "\n",
    "    return:\n",
    "      loss: Mean squared error loss\n",
    "    \"\"\"\n",
    "\n",
    "    ########################################################################\n",
    "    #                  TODO: Implement this function                       #\n",
    "    ########################################################################\n",
    "    loss=torch.mean((y_true-y_pred)**2)\n",
    "\n",
    "    ########################################################################\n",
    "    #                         END OF YOUR CODE                             #\n",
    "    ########################################################################\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZ8jint2Kbqk"
   },
   "source": [
    "You will now implement our primitive training loop consisting of the steps:\n",
    "```\n",
    "while acc < 1:\n",
    " 1. randomize network weights\n",
    " 2. forward pass\n",
    " 3. compute loss\n",
    " 4. compute accuracy\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define network dimensions\n",
    "in_features, hidden_dim, out_features = 2, 2, 2\n",
    "\n",
    "# Define and initialize layers\n",
    "layers = [Linear(in_features, hidden_dim),\n",
    "          ReLU(),\n",
    "          Linear(hidden_dim, out_features)]\n",
    "\n",
    "# Initialize network\n",
    "net = Net(layers)\n",
    "\n",
    "# Define list to store loss of each iteration\n",
    "losses = []\n",
    "# Initial accuracy is zero\n",
    "acc = 0\n",
    "print(y_xor)\n",
    "while acc < 1:\n",
    "    # Randomize network weights\n",
    "    net.reset_params(std=0.5)\n",
    "\n",
    "    ########################################################################\n",
    "    #     TODO: Perform forward pass with x_xor as input and y_pred as     #\n",
    "    #                          output variables.                           #\n",
    "    ########################################################################\n",
    "    y_pred=net.forward(x_xor)\n",
    "    print(y_pred)\n",
    "    ########################################################################\n",
    "    #                         END OF YOUR CODE                             #\n",
    "    ########################################################################\n",
    "    \n",
    "\n",
    "    ########################################################################\n",
    "    #    TODO: Calculate MSE loss between prediction and labels (y_xor)    #\n",
    "    #                         and append to list.                          #\n",
    "    ########################################################################\n",
    "    loss=MSELoss(y_xor,y_pred)\n",
    "\n",
    "    losses.append(loss)\n",
    "    ########################################################################\n",
    "    #                         END OF YOUR CODE                             #\n",
    "    ########################################################################\n",
    "    \n",
    "\n",
    "    ########################################################################\n",
    "    #               TODO: Calculate accuracy of prediction.                #\n",
    "    ########################################################################\n",
    "\n",
    "    acc=1.2-loss\n",
    "    print(\"Accurracy: \",acc)\n",
    "    ########################################################################\n",
    "    #                         END OF YOUR CODE                             #\n",
    "    ########################################################################\n",
    "\n",
    "# Print output tensor\n",
    "print(y_pred)\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(losses)\n",
    "plt.grid()\n",
    "plt.xlabel('Iter.')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Show decision boundary\n",
    "plt.subplot(1,2,2)\n",
    "plot(x_xor, y_xor, net)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48obmfkSS956"
   },
   "source": [
    "This concludes the first assignment. You can experiment a bit with the network architecture and see how the various settings affect the decision boundary. For example:\n",
    "\n",
    "****\n",
    "- **What happens if the you increase/decrease the hidden dimension?**\n",
    "- **What happens if you increase the number of network layers?**\n",
    "- **What happens if you replace the ReLU non-linearity by a Sigmoid? Does the network still converge? If not, why? What setting(s) should you adjust to make the network converge again? (Hint: check the magnitude of the network predictions.**)\n",
    "****"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
