{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/givani30/CS4240_DeepLearning/blob/main/Pytorch_tensors_a_small_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQKdh0x935hQ"
      },
      "source": [
        "## Install and import\n",
        "\n",
        "The python package name for Pytorch is `torch`. In order for it to be used, you must first of all **install** it. Please run the following lines of code to install it (or if already installed, check if it indeed is) and import it immediately after (including `numpy` and `matplotlib.pyplot` which will also be needed in this tutorial): \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNeZU-0ypmu6"
      },
      "source": [
        "!pip3 install torch\n",
        "\n",
        "import torch                    #pytorch\n",
        "import numpy as np              #numpy --> working with numpy arrays is a prerequisite for this introduction\n",
        "import matplotlib.pyplot as plt #matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZDN0sPcsNIN"
      },
      "source": [
        "## 1D tensors introduction\n",
        "\n",
        "Let's start simple and introduce how to instantiate a 1D-tensor and perform some relatively simple manipulations to them. \n",
        "\n",
        "- `torch.tensor([1,2,3])`: creates the following 64-bit 1-D tensor $[1,2,3]$\n",
        "- `torch.FloatTensor([1,2,3])`: creates the following 32-bit 1-D floating point tensor $[1.,2.,3.]$\n",
        "- type and data type can be checked using `.type` and `.dtype`\n",
        "- Slicing is similar to numpy :)\n",
        "- Re-arrangements are mostly done using `.view` (*to get a view!*, it shares the same underlying data with its base structure: avoids explicit data copying $\\rightarrow$ efficient)\n",
        "  - The shape (could) change: from 1-D to 5-D, 3-D to 2-D, etc.\n",
        "  - Shapes (of input and output) should be compatible: from (6) to (3,2) is possible, but to (4,5) is NOT.\n",
        "- Size can be checked with `.size` (similar to numpy, BUT it does retrieve the size *per dimension*)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InGaV6cFpsrj"
      },
      "source": [
        "#create 1D tensor in pytorch\n",
        "v = torch.tensor([1,2,3,4,5,6]) #creates int64-valued tensor\n",
        "\n",
        "print(v)\n",
        "print(v[:4])    #normal slicing, as in numpy\n",
        "print(v.dtype)  #torch.tensor creates int64-valued tensor\n",
        "print(v.type()) #type by default is LongTensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mrZ3_Glpyuj"
      },
      "source": [
        "#create floating point tensor\n",
        "f = torch.FloatTensor([1,2,3,4,5,6]) #creates float32-valued tensors\n",
        "\n",
        "print(f)\n",
        "print(f.dtype)  #floating point values\n",
        "print(f.type()) #LongTensor\n",
        "print(f.size()) #retrieves the size of the tensor - slightly different from numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQEH1-fLqK4C"
      },
      "source": [
        "#re-arrange tensors using tensor.view()\n",
        "v_6 = v.view(6,1)       #re-aranges the tensor (6 in row, 1 in column)\n",
        "v_minus = v.view(3, -1) #re-aranges the tensor (3 in row, -1 (=remainder) in column)\n",
        "\n",
        "print(v_6)\n",
        "print('')\n",
        "print(v_minus)\n",
        "print('')\n",
        "print(v_minus.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpnnpPtYNH1O"
      },
      "source": [
        "Conversions between numpy arrays and pytorch tensors often happens. It's really easy (in both directions):\n",
        "\n",
        "- `torch.from_numpy()`: from numpy array to pytorch tensor\n",
        "- `.numpy()`: from pytorch tensor to numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1z20PCmqNGa"
      },
      "source": [
        "#conversion between numpy and pytorch\n",
        "a = np.array([1,2,3,4,5])         #numpy array\n",
        "tensor_cnv = torch.from_numpy(a)  #from numpy.array to torch.tensor\n",
        "\n",
        "print(tensor_cnv)\n",
        "print(tensor_cnv.type()) #LongTensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3_IHCKlqyu7"
      },
      "source": [
        "numpy_cnv = tensor_cnv.numpy() #from torch.tensor to numpy.array\n",
        "print(numpy_cnv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqqsl3mKsQ56"
      },
      "source": [
        "## General operations on 1D tensors\n",
        "\n",
        "Now that we know how to create and manipulate tensors, we would actually like to do something with them. Addition, multiplication, dot-product, exponentials, sigmoids, etc. It's (almost) exactly the same as numpy. Have a look at the following examples. If you plot a tensor using `matplotlib.pyplot` make sure to convert the tensor to an actual perceivable array (such as numpy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkFPRHkpsGrw"
      },
      "source": [
        "#universal operations on tensors (highly similar to numpy)\n",
        "\n",
        "t_one = torch.tensor([1, 2, 3])\n",
        "t_two = torch.tensor([2, 7, 8])\n",
        "\n",
        "t_add  = t_one + t_two            #pointwise addition\n",
        "t_pm   = t_one * t_two            #pointwise multiplication\n",
        "t_dot  = torch.dot(t_one, t_two)  #dot product between two tensors\n",
        "\n",
        "print(t_add)\n",
        "print(t_pm)\n",
        "print(t_dot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG3P5vEAtqd2"
      },
      "source": [
        "#universal functions on tensors (highly similar to numpy)\n",
        "\n",
        "x = torch.linspace(0, 10, 100) #from 0 tot 10, number of steps by default: 100\n",
        "y = torch.exp(x)               #exponential\n",
        "\n",
        "plt.plot(x.numpy(), y.numpy()) #plots x vs y (don't forget to convert to array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5WZN-QUuQn2"
      },
      "source": [
        "##Higher dimensional tensors\n",
        "\n",
        "We have already seen some examples of 2-dimensional tensors - using the view function to reshape a 1-dimensional one. The dimension of a tensor can be checked using `.dim()` (can you think of another way?). Some extra functions and examples are provided below:   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F8W2kqouAh5"
      },
      "source": [
        "#creating 2D tensors\n",
        "one_d = torch.arange(2, 14, 1) #one d tensor from 2 to 13 (stops just before 14)\n",
        "two_d = one_d.view(6, -1)      #creates 2D-tensor: 6 x remainder(= 2)\n",
        "\n",
        "print(one_d)\n",
        "print(two_d)\n",
        "print(one_d.dim(), two_d.dim()) #prints the dimensions of both tensors (= 1, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slkzS-vTusNw"
      },
      "source": [
        "#slicing 2D tensors is similar to numpy\n",
        "\n",
        "print(two_d[2,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WF3WM2tvc0x"
      },
      "source": [
        "#creating 3D tensors\n",
        "\n",
        "three_d = torch.arange(18).view(3,3,2) #create 3D-tensor 3 x 3 x 2\n",
        "print(three_d) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN1mw0X4vjRZ"
      },
      "source": [
        "#slicing 3D tensors is similar to numpy\n",
        "\n",
        "#  1. first the blocks (in this case consisting of 6 elements (or 3 x 2 tensors) each)\n",
        "#  2. second the row \n",
        "#  3. finally the column\n",
        "\n",
        "#if we want to find 14 --> 1. [2], 2. [1], 3. [0]\n",
        "print(three_d[2, 1, 0])\n",
        "\n",
        "#if we want the whole 2nd block \n",
        "print(three_d[1, :, :]) #: for whole range"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggX-o29mQfiT"
      },
      "source": [
        "Matrix multiplication - NOT pointwise, but by structure - is the (THE!) working horse in deep learning: layer-to-layer connections are efficiently implemented using it. Luckily, it is really convenient how to do so and takes only a single small line using `torch.matmul(A, B)` which specifically implements $\\mathbf{A} \\cdot \\mathbf{B}$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew2K-YJhv-Vi"
      },
      "source": [
        "#matrix multiplication\n",
        "\n",
        "mat_a = torch.rand(8).view(4,2) #random (uniformly distributed on [0, 1)) 4 x 2 matrix\n",
        "mat_b = torch.rand(8).view(2,4) #random 2 x 4 matrix\n",
        "\n",
        "#torch.mm (until 2D) and torch.matmul (all D)\n",
        "mat_mm = torch.matmul(mat_a, mat_b) #matrix multiplication --> mat_a times mat_b\n",
        "\n",
        "print(mat_a); \n",
        "print('')\n",
        "print(mat_b); \n",
        "print('')\n",
        "print(mat_mm) #should be 4x4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuHbxGdw0Fhi"
      },
      "source": [
        "## Derivatives: the awesome embedded `.backward` function\n",
        "\n",
        "**Derivates** are essential for the second tribe - the connectionists - of machine learning scientists. (You thought that only neural networks are crucial in machine learning? Yes, they are a big and provably crucial part, but there are other tribes (the symbolists, evolutionaries, bayesians, and analogizers) that develop learners in completely different ways - and yes there are people trying to combine both\\*). Let's not drift off and keep focused. \n",
        "\n",
        "Essential? Neural networks update their parameters using their gradients (with respect to some cost function). Luckily, again, Pytorch makes our lives terribly easy and intuitive. Say you have some function $y$ which is some function of $x$:\n",
        "\n",
        "$$ y = f(x) $$\n",
        "\n",
        "If specified, `y.backward` finds that derivative, $\\frac{dy}{dx}$ and `x.grad` finds the value, $\\frac{dy}{dx} \\left( x\\right)$,  for the gradient at the point x. If a variable depends on more than one variable, a similar strategy can be used - including partial derivates (see the example in the second code block). Concerning neural networks, what could $y$ and $x$ represent here? \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx5n5a_Sxtyx"
      },
      "source": [
        "# normal derivative\n",
        "x = torch.tensor(2.0, requires_grad = True) #gradient w.r.t. to x can only be computed as required_grad = True (False by default)\n",
        "y = 9*x**3 + 2*x**2                         #9x^3 + 2x^2\n",
        "\n",
        "y.backward()  #finds derivative w.r.t. its arguments, which are only x in this case\n",
        "print(x.grad) #finds the value dy/dx (at x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pnCuh2M0S-p"
      },
      "source": [
        "#partial derivatives\n",
        "\n",
        "z = torch.tensor(3.0, requires_grad = True) #create new variable z\n",
        "y = 9*x**3 + 2*z**2                         #9x^3 + 2z^2\n",
        "\n",
        "y.backward()\n",
        "print(x.grad) #dy/dx (at x, z)\n",
        "print(z.grad) #dy/dz (at y, z)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}